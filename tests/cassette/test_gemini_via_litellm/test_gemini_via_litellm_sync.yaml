interactions:
- request:
    body: '{"contents": [{"role": "user", "parts": [{"text": "ping ping"}]}], "generationConfig":
      {"temperature": 0, "top_p": 1, "max_output_tokens": 50}}'
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '143'
      content-type:
      - application/json
      host:
      - generativelanguage.googleapis.com
      user-agent:
      - litellm/1.53.7
    method: POST
    uri: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyAQeZapzjgQGnx_mnMCRuKrVNGpe2T3sMI
  response:
    content: "{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\":
      [\n          {\n            \"text\": \"Pong pong\\n\"\n          }\n        ],\n
      \       \"role\": \"model\"\n      },\n      \"finishReason\": \"STOP\",\n      \"avgLogprobs\":
      -0.6001678307851156\n    }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\":
      2,\n    \"candidatesTokenCount\": 3,\n    \"totalTokenCount\": 5,\n    \"promptTokensDetails\":
      [\n      {\n        \"modality\": \"TEXT\",\n        \"tokenCount\": 2\n      }\n
      \   ],\n    \"candidatesTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n
      \       \"tokenCount\": 3\n      }\n    ]\n  },\n  \"modelVersion\": \"gemini-2.0-flash\"\n}\n"
    headers:
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json; charset=UTF-8
      Date:
      - Thu, 24 Apr 2025 13:05:40 GMT
      Server:
      - scaffolding on HTTPServer2
      Server-Timing:
      - gfet4t7; dur=1085
      Transfer-Encoding:
      - chunked
      Vary:
      - Origin
      - X-Origin
      - Referer
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - SAMEORIGIN
      X-XSS-Protection:
      - '0'
    http_version: HTTP/1.1
    status_code: 200
- request:
    body: '{"contents": [{"role": "user", "parts": [{"text": "ping"}]}], "generationConfig":
      {"temperature": 0, "top_p": 1, "max_output_tokens": 50}}'
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '138'
      content-type:
      - application/json
      host:
      - generativelanguage.googleapis.com
      user-agent:
      - litellm/1.53.7
    method: POST
    uri: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=AIzaSyAQeZapzjgQGnx_mnMCRuKrVNGpe2T3sMI
  response:
    content: "{\n  \"candidates\": [\n    {\n      \"content\": {\n        \"parts\":
      [\n          {\n            \"text\": \"Pong! \\n\\nIs there anything I can
      help you with?\\n\"\n          }\n        ],\n        \"role\": \"model\"\n
      \     },\n      \"finishReason\": \"STOP\",\n      \"avgLogprobs\": -0.17323689801352365\n
      \   }\n  ],\n  \"usageMetadata\": {\n    \"promptTokenCount\": 1,\n    \"candidatesTokenCount\":
      14,\n    \"totalTokenCount\": 15,\n    \"promptTokensDetails\": [\n      {\n
      \       \"modality\": \"TEXT\",\n        \"tokenCount\": 1\n      }\n    ],\n
      \   \"candidatesTokensDetails\": [\n      {\n        \"modality\": \"TEXT\",\n
      \       \"tokenCount\": 14\n      }\n    ]\n  },\n  \"modelVersion\": \"gemini-2.0-flash\"\n}\n"
    headers:
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
      Content-Encoding:
      - gzip
      Content-Type:
      - application/json; charset=UTF-8
      Date:
      - Thu, 24 Apr 2025 13:05:40 GMT
      Server:
      - scaffolding on HTTPServer2
      Server-Timing:
      - gfet4t7; dur=1198
      Transfer-Encoding:
      - chunked
      Vary:
      - Origin
      - X-Origin
      - Referer
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - SAMEORIGIN
      X-XSS-Protection:
      - '0'
    http_version: HTTP/1.1
    status_code: 200
version: 1
